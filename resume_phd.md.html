<meta charset="utf-8" emacsmode="-*- markdown -*-">

APPRENTISSAGE ROBUSTE AU TRANSFERT, AVEC APPLICATION EN PHYSIQUE DES HAUTES ÉNERGIES
===========================

Robust transfer learning, with application to high energy physics
===========================

Systematic aware particle physics modeling with machine learning
===========================

### Keywords :

Machine learning
transfer learning
simulateur
big data
Statistical Estimation
Physical modeling
Systematic error, systematic bias
Counting problems
Poisson process
Data driven "dimension reduction" vs knowledge driven
Optimization
Calibration
Dataset wise neural network / permutation invariant neural network
Learning to discover => learning to measure/learning to count
LHC
particle physics
Higgs Boson
High Energy Physics


BACKGROUND AND MOTIVATIONS:
---------------------------


The objective of this thesis is to leverage novel machine learning methods to reduce systematic errors introduced in the complex physics modeling and simulation of particule physics pipelines, like those of the Large Hadron Collider.

The central problem we adress is that of estimating
the branching factor of Higgs to tau tau, which can be brought back to counting a number of rare events amount a very large number of background events of lesser interest.
This problem, which involves building and using accurate simulators, is fraught with modeling and computational challenges due to the ever increasing number of detectors (hundreds of millions in the newests colliders). In particular, avoiding, diagnozing, reducing, and compensating
the impact of nuisance parameters has become one of the bottlenecks and will be the focus of our attention.


Although it has been addressed in the past in several manners, including calibrating simulators with real data from independent measurements (citation), projecting data on hand-crafted feature(s) designed by an expert to factor out systematic effects (citation), and, more recently, training a learning machine with synthetic data to optimize such projection(s) (cite Pivot and Inferno), such methods have a number of limitations: the two former are man-power intensive, require domain knowledge, and may discard valuable information invisible to the human eye; the latter have shown promises on toy problems, but make strong hypotheses not verified in practice, such as separability of the effect of the parameter of interest and the nuisance parameters.


We tackle novel aspects of this last line of research from three points of view: conceptual, algorithmic, and technical. Conceptually, we make connections between calibration of simulators and transfer learning, we clarify that systematic aware modeling implies disentangling nuisance factors as opposed to enforcing invariance to nuisance factors. We put in practice such concepts by designing learning machines, trained on simulated data, which predict nuisance factors and factor them into the prediction of the parameter of interest, when applied to real data.

Our contributions include proposing proposing novel algorithms to take into account systematic errors and benchmarking them against established methods, such as the ones mentionned before. We leverage our experience with the organization and analysis of the higgsML challenge, including the dataset, and evaluation methods, as well as collaborations with the LAL.


MATERIAL AND METHODS:
---------------------

[Ici pose les questions et on décrit comment on va y répondre]

To demonstrate advances on this problem, we use
the publicly available higgsML challenge
“real” data (data obtained with a realistic simulator),
enhanced with a re-simulation process, developped for this project,
recomputing all the attributes as if the full simulation were re-ran with different input parameters.
[trop detaillé?]

We also conduct systematic experiments on a set of smaller didactic examples to clarify the following points :
- sustainability of direct inference using modern neural network architecture (for "easy" problems only ?).
- is using a poisson count process instead of the generative model preventing to reach best possible performances [peu d'intérêt ?]

We benchmark 8 methods on a 30+ GPU cluster.
Including
standard classifiers, data augmentation, tangent propagation, pivotal neural network, inference aware neural optimization network, and direct regression.
For which we compare produced estimators with the standard pipeline alone to the pipeline using additional nuisance parameter related data mining.


[Qu'est ce qu'on a comme résultats théoriques ?]

In addition, several theoretical insights are obtained by analyzing successes and failures in numerical experiments.
We derived several properties of
[… ???]

using mathematics of statistics techniques
[… ???]


RESULTS:
--------

[Ici on répond aux questions posées avant]

[Using predictions of the nuisance parameters helps the estimations]
Our results on realistic synthetic data demonstrate that
combining the standard method, optimized to extract parameter of interest related summary statistics, and nuisance parameter oriented data mining reduces uncertainty of the estimators.

[être + précis sur le pipeline hybrid, c'est trop générique/flou]
[Les ingrédients et comment ils se complémentent]

One particular highlight is:
[...]

This opens the door to:
usage of an hybrid pipeline using the direct inference to mine information about the nuisance parameters from the data.
But also more recent research around uncertainty estimations of neural network outputs to improve the direct inference pipeline.


the suitability of direct regression for inference. Although direct inference may suffer from overconfidence in a small samples regime.


This prompted us to make a more in depth and detailed study using artificial data, which revealed:
[...]

This leads us to conclude with confidence that
extracting more information about the nuisance parameters always improve the resulting confidence interval.
This is the main result of this thesis.

We also conducted an analysis of the computational efficiency of the algorithms revealing that:
[refaire : Pivot + tangent prop + DA = Inforcing invariance]
tangent propagation suffers from the lack of jacobian vector product in common libraries,
INFERNO is (FIXME : still true ???) very accurate,
data augmentation shows good accuracy (FIXME : still true ???) and does not requires a differentiable generator,
and although direct regression is compuationnaly intensive for training its inference speed is unmached.


CONCLUSION AND FURTHER WORK:
----------------------------

[Ici c'est : tel résultat a montré que ... et donc on doit faire de tel manière]

This work allowed us to advance the field of
systematic aware learning
in a number of directions, including
insights that disentangling the parameters of interest from nuisance parameters is more valuable than invariance of systematic effect induces by those nuisance parameters,
and the importance of data mining nuisance paramters related information to impove modelization.


The framework we proposed helped us clarify and unify past work and allowed us to identify critics difficulties not anticipated, such as
reaching an acurate measuring of the confidence interval which requires many parameter and algorithm adjustment for minimization;

[… ] Likelihood free method ?

[… ]

With perseverance, we managed to understand such issues and resolve a number of them using
a heuristic to choose the number of bins, adjust the minimal precision of the minimizer, using multiple minimization algorithm together to automatically reach the minimum;

[… ] propose a direct aproach using modern neural network architecture to train a regressor on a dataset instead of individual instances;

But much work remains to be done to improve
integration of the final objective figure of merit when the simulator is not differentiable,
automating relevant summary statistics extraction,
and ensuring the quality of the uncertainty predicted by neural network direct regression.

Further research directions include
extacting more information from the simulator (madminer/mining gold),
ensemble of neural networks for better measurement of the uncertainty (stochastic weight averaging seems promissing),
and robustness to nuisance parameters in a likelihood free inference framework.



MAIN CONTRIBUTIONS (at least 5 points):
---------------------------------------

In summary, the main contributions of this thesis are:

- An improved pipeline using additional data mining.
It is possible to improve inference by using a dataset wise neural network regressor to measure the nuisance parameters from experimental data .
- Implementation of a realistic fast re-simulator leveraging GPU computation.
- Technics to implement the standard pipeline and automate it.
Like the algorithm chain to help the minimization step.
Or the heuristic to choose the number of bins.
(Or the details around training some of the compared ML models.)
- Experimental evidence against systematic invariance methods.
The objective of the inference is disentangling the parameter of interest from the nuisance parameters.
- Proof of concept using direct regression using modern neural network architectures.
The speed of inference and simplicity of this direct pipeline comes at the expense of theoretical guarantees on the coverage of the confidence interval.



Notes:
---------------------------------------

Juste un coin où noter les choses utiles au résumé

### La liste des questions et leur réponses

Ici je rassemble la liste des questions auxquelles nos travaux répondent clairement.

1. Comment intégrer + de features pour améliorer les estimateurs tout en limitant l'impact (négatif) des paramètres de nuisance qui les accompagnent ?
    - Graçe à un data mining visant spécifiquement les paramètres de nuisance pour acompagner les summary statistics, qui elles visent spécifiquement le paramètre d'intérêt
    - Rassembler ces deux informations améliore l'estimation
    - réduction de l'interval de confiance sur le Toy 3D
    - résultats similaires sur HiggsML en cour de solidification mais à l'oeil ça a l'ai bon.
2. Est-ce que rendre robuste aux effets (géométriques induit par les) systématiques améliore systématiquement les estimateurs  ?
    - S'il existe une projection (ou transformation) des données pour laquelle le paramètre d'intérêt et les paramètres de nuisance sont indépendants alors Oui (citation Pivot et success story de Pivot).
    - Sinon la qualité des estimateurs sera insensible aux efforts fournis.
3. Est-ce que l'on peut remplacer toute la chaine amenant à l'estimation par un apprentissage supervisé ?
    - En utilisant le simulateur pour générer plein des paires (dataset, mu) au lieu de (event, label) on peut entrainer un réseaux de neuronne pour estimer directement mu
    - :-)  C'est complètement domain knowledge free
    - :-( Pas de garantie théorique forte sur l'estimation de l'incertitude sur mu


### Autres observations

Ici je rassemble les autres observations moins intéréssantes.
Peut-être à garder pour la partie discussion ou autre.
Bref que je ne juge pas utile pour le résumé.

- La vielle méthode consistant à ne choisir qu'une feature et faire notre histogram dessus reste assez performante même si elle est systématiquement moins bonne qu'un simple classifieur
    - Elle reste utile si l'on a besoin de résultat facile et rapide par exemple.
- La partie technique comporte beaucoup de détails importants !
    - Mettre en garde de futurs travaux : mesurer correctement l'intervalle de confiance demande de l'attention
    - Il faut éviter des conclusions faussées par :
        - Une mauvaise optimisation
        - Une fuite de donnée du simulateur
        - ?
- using a simplified statistical model (Binned Poisson count process) allows us to reach the same performances as using the true generative model. [ré-expliqué mieux]


<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'none'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
