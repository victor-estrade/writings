APPRENTISSAGE ROBUSTE AU TRANSFERT, AVEC APPLICATION EN PHYSIQUE DES HAUTES ÉNERGIES
===========================

Robust transfer learning, with application to high energy physics
===========================

Systematic aware particle physics modeling with machine learning
===========================

### Keywords :

Machine learning
transfer learning
simulateur
big data
Statistical Estimation
Physical modeling
Systematic error, systematic bias
Counting problems
Poisson process
Data driven "dimension reduction" vs knowledge driven
Optimization
Calibration
Dataset wise neural network / permutation invariant neural network
Learning to discover => learning to measure/learning to count
LHC
particle physics
Higgs Boson
High Energy Physics


BACKGROUND AND MOTIVATIONS:
---------------------------


The objective of this thesis is to leverage novel machine learning methods to reduce systematic errors introduced in the complex physics modeling and simulation of particule physics pipelines, like those of the Large Hadron Collider.

The central problem we adress is that of estimating
the branching factor of Higgs to tau tau, which can be brought back to counting a number of rare events amount a very large number of background events of lesser interest.
This problem, which involves building and using accurate simulators, is fraught with modeling and computational challenges due to the ever increasing number of detectors (hundreds of millions in the newests colliders). In particular, avoiding, diagnozing, reducing, and compensating
the impact of nuisance parameters has become one of the bottlenecks and will be the focus of our attention.


Although it has been addressed in the past in several manners, including calibrating simulators with real data from independent measurements (citation), projecting data on hand-crafted feature(s) designed by an expert to factor out systematic effects (citation), and, more recently, training a learning machine with synthetic data to optimize such projection(s) (cite Pivot and Inferno), such methods have a number of limitations: the two former are man-power intensive, require domain knowledge, and may discard valuable information invisible to the human eye; the latter have shown promises on toy problems, but make strong hypotheses not verified in practice, such as separability of the effect of the parameter of interest and the nuisance parameters.


We tackle novel aspects of this last line of research from three points of view: conceptual, algorithmic, and technical. Conceptually, we make connections between calibration of simulators and transfer learning, we clarify that systematic aware modeling implies disentangling nuisance factors as opposed to enforcing invariance to nuisance factors. We put in practice such concepts by designing learning machines, trained on simulated data, which predict nuisance factors and factor them into the prediction of the parameter of interest, when applied to real data.

Our contributions include proposing proposing novel algorithms to take into account systematic errors and benchmarking them against established methods, such as the ones mentionned before. We leverage our experience with the organization and analysis of the higgsML challenge, including the dataset, and evaluation methods, as well as collaborations with the LAL.


MATERIAL AND METHODS:
---------------------

To demonstrate advances on this problem, we use
the publicly available higgsML challenge
“real” data (data obtained with a realistic simulator),
enhanced with a re-simulation process, developped for this project,
recomputing all the attributes as if the full simulation were re-ran with different input parameters.
[trop detaillé?]

We also conduct systematic experiments on a set of smaller didactic examples to clarify the following points :
- is using a poisson count process instead of the generative model preventing to reach best possible performances
- sustainability of direct inference using modern neural network architecture.

We benchmark 8 methods. Including
standard classifiers, data augmentation, tangent propagation, pivotal neural network, inference aware neural optimization network, and direct regression.
The experiments are carried out on a 30+ GPU cluster.

In addition, several theoretical insights are obtained by analyzing successes and failures in numerical experiments.
We derived several properties of
[… ???]

using mathematics of statistics techniques
[… ???]


RESULTS:
--------

[Ici on répond aux questions posées avant]

Our results on realistic synthetic data demonstrate that
using a simplified statistical model (Binned Poisson count process) allows us to reach the same performances as using the true generative model.
[ré-expliqué mieux]

One particular highlight is:
[Using predictions of the nuisance parameters helps the estimations]
the suitability of direct regression for inference. Although direct inference may suffer from overconfidence in a small samples regime.

This opens the door to:
usage of an hybrid pipeline using the direct inference to mine information about the nuisance parameters from the data.
But also more recent research around uncertainty estimations of neural network outputs to improve the direct inference pipeline.

[être + précis sur le pipeline hybrid, c'est trop générique/flou]
[Les ingrédients et comment ils se complémentent]

This prompted us to make a more in depth and detailed study using artificial data, which revealed:
that this hybrid pipeline is improving the inference (size of the confidence interval) for both the paramter of interest and the nuisance parameters.

This leads us to conclude with confidence that
extracting more information about the nuisance parameters always improve the resulting confidence interval.
This is the main result of this thesis.

We also conducted an analysis of the computational efficiency of the algorithms revealing that:
[refaire : Pivot + tangent prop + DA = Inforcing invariance]
tangent propagation suffers from the lack of jacobian vector product in common libraries,
INFERNO is (FIXME : still true ???) very accurate,
data augmentation shows good accuracy (FIXME : still true ???) and does not requires a differentiable generator,
and although direct regression is compuationnaly intensive for training its inference speed is unmached.


CONCLUSION AND FURTHER WORK:
----------------------------

[Ici c'est : tel résultat a montré que ... et donc on doit faire de tel manière]

This work allowed us to advance the field of
systematic aware learning
in a number of directions, including
insights that disentangling the parameters of interest from nuisance parameters is more valuable than invariance of systematic effect induces by those nuisance parameters,
and the importance of data mining nuisance paramters related information to impove modelization.


The framework we proposed helped us clarify and unify past work and allowed us to identify critics difficulties not anticipated, such as
reaching an acurate measuring of the confidence interval which requires many parameter and algorithm adjustment for minimization;

[… ] Likelihood free method ?

[… ]

With perseverance, we managed to understand such issues and resolve a number of them using
a heuristic to choose the number of bins, adjust the minimal precision of the minimizer, using multiple minimization algorithm together to automatically reach the minimum;

[… ] propose a direct aproach using modern neural network architecture to train a regressor on a dataset instead of individual instances;

But much work remains to be done to improve
integration of the final objective figure of merit when the simulator is not differentiable,
automating relevant summary statistics extraction,
and ensuring the quality of the uncertainty predicted by neural network direct regression.

Further research directions include
extacting more information from the simulator (madminer/mining gold),
ensemble of neural networks for better measurement of the uncertainty (stochastic weight averaging seems promissing),
and robustness to nuisance parameters in a likelihood free inference framework.



MAIN CONTRIBUTIONS (at least 5 points):
---------------------------------------

In summary, the main contributions of this thesis are:

- Implementation of a realistic fast re-simulator leveraging GPU computation.
- Technics to implement the standard pipeline and automate it.
Like the algorithm chain to help the minimization step.
Or the heuristic to choose the number of bins.
(Or the details around training some of the compared ML models.)

- Experimental evidence against systematic invariance methods.
The objective of the inference is clother to disentangling the parameter of interest from the nuisance parameters.

- Proof of concept using direct regression using modern neural network architectures.
The speed of inference and simplicity of this direct pipeline comes at the expense of theoretical guarantees on the coverage of the confidence interval.

- An improved pipeline using additional data mining.
It is possible to improve inference by using a dataset wise neural network regressor to measure the nuisance parameters from experimental data .

- No free lunch : On the realistic problem the old ways using just one feature still offer competitive performance.
