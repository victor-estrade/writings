<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Claim : mining for nuisance parameters improves inference**

This document gather evidence that mining information about the nuisance parameter in the dataset improves inference.
Also note that it seems that some methods (INFERNO and Blind regressor) already exploit this advantage although they are not explicitly design for it.



Core idea
===============================================================================

The idea originally comes from a naive thought :
> If the systematic effect significantly impacts the data then we can *see* it.
> If we can *see* it then we can measure it.
> If it is measurable, then it can be compensated.
> if we can compensate it then there is no more systematic effect.

Of course this works only if the systematic effect and the studied phenomenom impact can be distinguished.
In real life it is impossible to fully the disentangle the nuisance parameters and parameter of interest impact on data.

But the classic pipeline is optimized in a seeting without systematic effect.
The summary statistic ($n_i$) are especially optimized to contain most (if not all) the information about the parameter of interest.
But the information about the nuisance parameters is lost in the process and we have to rely on another measurement : the calibration.

Based on this observation, we propose to also extract information about nuisance parameter from the data.
The likelihood of the generating process is then updated to take this information into account.
The hope is that these new information improves inference ie reduced bias or shorten the confidence interval.



Results on the 1D toy
===============================================================================

Reduces the bias for classifiers
-------------------------------------------------------------------------------

![Figure [fig:NN-prior]: average of estimators from many Neural network classifier (different hyper-parameters) on 1D toy. Colors are related to the the true value of $\alpha$](./claim_mining_for_nuisance/NN_prior_profusion_true_mu_target_mean.png)


As seen in fig. [fig:NN-prior] the classifier are biased.

![Figure [fig:NN-calib]: average of estimators from many Neural network classifier (different hyper-parameters) on 1D toy. Colors are related to the the true value of $\alpha$](./claim_mining_for_nuisance/NN_calib_profusion_true_mu_target_mean.png)

But in fig. [fig:NN-calib] the bias is almost zero.


This phenomenon is also present with regularized classifier as seen in Figure [fig:DA-prior] to Figure [fig:Pivot-calib]

![Figure [fig:DA-prior]: Data augmentation (classic)](./claim_mining_for_nuisance/DA_prior_profusion_true_mu_target_mean.png) ![Figure [fig:DA-calib]: Data augmentation (upgrade)](./claim_mining_for_nuisance/DA_calib_profusion_true_mu_target_mean.png)
![Figure [fig:Pivot-prior]: Pivot (classic)](./claim_mining_for_nuisance/Pivot_prior_profusion_true_mu_target_mean.png) ![Figure [fig:Pivot-calib]: Pivot (upgrade)](./claim_mining_for_nuisance/Pivot_calib_profusion_true_mu_target_mean.png)

And the Parametrised-regresor.

![Figure [fig:Param-Reg-prior]: Param-Regressor (classic)](./claim_mining_for_nuisance/Param-Reg_prior_profusion_true_mu_target_mean.png) ![Figure [fig:Param-Reg-calib]: Param-Regressor (upgrade)](./claim_mining_for_nuisance/Param-Reg_calib_profusion_true_mu_target_mean.png)



Results on higgs
===============================================================================





Methods doing it anyway
===============================================================================

!!! error
    ugly title


Blind regressor
-------------------------------------------------------------------------------

Of course the Blind regressor ignores the calibration inforamtion and only uses the empirical distribution.
It is natural to think that such method is incline to internally measure the nuisance parameter of at least disentangle it from its target : the parameter of interest.




Inferno
-------------------------------------------------------------------------------

Inferno could use some of the bins to be very sensitive to the nuisance parameter.
If some $n_i$ are very sensitye to $\alpha$ then the maximizing the likelihood requires to be close to the true $\alpha$.

!!! error
    C'est un peu maladroitement expliqu√©...

It seems that inferno is not biased on the 1D toy.
But also Inferno does not improves much from this new information.
So maybe it is because it already *knows* about it.




<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
