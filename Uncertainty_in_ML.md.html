<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Uncertainty in ML**


Methods to predict uncetainty
===============================================================================

Here is a list of methods to predict uncertainty of machine learning models.
We limit ourselves to neural network methods.

The methods are not described in details.
This parts gather comments on the proposed methods.


Mixture density networks
-------------------------------------------------------------------------------

First occurence in [#Bishop1994].
Maybe more recent developement in [#PhDPapamakarios].
Some recent use for inference of SIR model params [#Dav2020] (for fun).

The objective is to build a probability distribution of the random variable target|data.
Since the link between target and data is not known exactly is will be approximated by a neural netork.
The general idea is to train neural network as a maximum likelihood estimator of a gaussian distribution.
Any other assumed distribution of the random variable target|data can be used as long as it is differentiable.

This method was initially proposed as a generatilzation of MSE regression loss.
MSE is a particular case of Gaussian NLL with unit variance.

More details can be found in other documents.


Bayesian networks
-------------------------------------------------------------------------------

!!! error
    TODO Trouver papiers (Alex Graves 2015 ?) et les travaux suivants



Quantile regression
-------------------------------------------------------------------------------

Cited in [#Blog1Part3].

Quantil regression core idea is to train regressors to predict quantiles instead of the average target value.

The training technics I found were merely training regressors on quantiles computed from the dataset.
Unfortunately finding those quantiles is extremely data intensive especially in high dimensions (histograms).

Moreover if it is possible to build histograms of target|data then why not directly using the histogram as a density estimator ?
Or even better density estimators ?

This method does not seems to be very usefull in practice.



MC Dropout
-------------------------------------------------------------------------------

Method described in [#Gal2015] using dropout layers to measure uncertainty of the output.

Although the uncertainty prediction is not clear to me it is very simple to implement.
Did not read the bayesian approx / gaussian process part in details.




Deep ensembles
-------------------------------------------------------------------------------

Described in [#Lak2017].

It gathers many technics together to predict target|data probability density.

- Mixture Density Networks
- Adversarial samples data augmentation to smooth prediction (in order to counter possible overfitting of MDN)
- Ensemble networks trained with various initialization and data shuffling.

Any statement about neural network overfitting is suspicious to me.
Neural network can overfit but expermiments showed that sometimes it does and sometimes it doesn't.
But also that intuition about overfitting is sometimes refuted by experiments.
So I am sceptical about this adversarial data augmentation step.

The ensemble part is computationaly expensive although is can be concurent.
Training N independant neural network is not complex but still quite expensive.

!!! warning
    Reminder : the formulas for the total variance is not the mean of the variances of the MDN !




Swag ?
-------------------------------------------------------------------------------

Victor presented a seminar about [#Wilson2020].
It is mostly about generalization.
It includes prediction as probability densities which is related to many method to predict uncertainty.

Looking more into it leads to a blog [#PytorchSwagBlog] and the paper [#Mad2019] describing Stochastic weigh averaging (Gaussian) method.
It seems to be a more recent way of prediction uncertainty.



Comments
-------------------------------------------------------------------------------

MC-Dropout, Lak2017 and Swag are basically using ensembles of neural networks variance to estimate uncertainty.

- MC-Dropout is using dropout layer to produce sub-networks.
- Deep ensembles is training networks with various initialization and shuffling samples
- Swag is sampling networks parameters around the minimum of a pretrained network


Swag and Deep ensembles can be combined to give Deep ensemble Swag trained neural networks.

The comparison of Swag and Deep Ensembles is suspicious.
In the paper [#Mad2019] Deep ensembles is given only 3 or 5 neural net which is a very small ensemble at first glance.
But Deep ensemble paper [#Lak2017] also uses 5 networks for the real regression proble (sec 3.3)...




Higgs
-------------------------------------------------------------------------------


These methods could be add to the benchmark.
But none of them will solve the issue on Higgs in the S << B regime for which the regressor is simply not able to train.




Methods to evaluate uncertainty predictions
===============================================================================

The idea is to answer to the question :
*How trustworthy are the uncertainty estimates of proposed methods ?*

But all the proposed metrics evaluates accuracy under uncertainty instead.
The metrics measure accuracy and penalise over-confidence and under-confidence of the prediction.


Classification
-------------------------------------------------------------------------------

In classification the output is a finite array of probability making scores quite easy.
The Brier score simply sums/average the probability given to the true class.

Negative log likelihood usually leads to cross-entropy loss.



Regression
-------------------------------------------------------------------------------

For regression (normalized) Mean Squared Error and Negative Log Likelihood are also the most commons scores.

eval = sum/average of predicted_NLL(target | data)

If target|data is indeed a random variable with non-zero variance then this methods perfectly makes sense.

Comments
-------------------------------------------------------------------------------

Many experiments are about classifications.
It is easier to measure probabilities in the discrete world of classification.

Unfortunately our problem is a regression problem.
Classification methods are only used as a proxy to perform dimension reduction.

The mostly used metrics measure accuracy and penalise over-confidence and under-confidence of the prediction.
But neither NLL or MSE gives a numerical answer to the question :
*How trustworthy are the uncertainty estimates of proposed methods ?*
Which is a question about the error of the uncertainty (error on error).


Links and biblio
===============================================================================

Evaluation methods
-------------------------------------------------------------------------------

[#Challenge2005]: Evaluating Predictive Uncertainty ftp://nozdr.ru/biblio/kolxoz/Cs/CsLn/Machine%20Learning%20Challenges,%201%20conf.,%20MLCW%202005(LNCS3944,%20Springer,%202006)(ISBN%203540334270)(473s).pdf#page=12

[#Ova2019]: Can you trust your model's uncertainty ? https://arxiv.org/abs/1906.02530


Mixture density networks
-------------------------------------------------------------------------------

[#Bishop1994]: Mixture density networks https://publications.aston.ac.uk/id/eprint/373/1/NCRG_94_004.pdf

[#PhDPapamakarios]: Neural Density Estimation and Likelihood-free Inference https://arxiv.org/pdf/1910.13233.pdf

[#Dav2020]: The use of mixture density networks in the emulation of complex epidemiological individual-based models https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006869


Dropout
-------------------------------------------------------------------------------

[#Gal2015]: Dropout as Bayesian Approximation  https://arxiv.org/pdf/1506.02142.pdf and appendix : https://arxiv.org/pdf/1506.02157.pdf


Ensemble
-------------------------------------------------------------------------------

[#Lak2017]: Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles https://arxiv.org/abs/1612.01474

Swag
-------------------------------------------------------------------------------

[#PytorchSwagBlog]: https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/

[#Mad2019]: A Simple Baseline for Bayesian Uncertainty in Deep Learning https://arxiv.org/abs/1902.02476


Unsorted
-------------------------------------------------------------------------------

[#Sha2018]: A Less Biased Evaluation of Out-of-distribution Sample Detectors https://arxiv.org/abs/1809.04729

[#SomeSlides]: https://cs.adelaide.edu.au/~javen/talk/ML05_Uncertainty_in_DL.pdf

[#Loq2020]: A General Framework for Uncertainty Estimation in Deep Learning  https://arxiv.org/pdf/1907.06890.pdf

[#Gho2020]: Estimating Uncertainty in Deep Learning for Reporting Confidence: An Application on Cell Type Prediction in Testes Based on Proteomics https://link.springer.com/chapter/10.1007/978-3-030-44584-3_18

[#Blog1Part1]: Estimating Uncertainty in Machine Learning Models - Part 1 https://towardsdatascience.com/estimating-uncertainty-in-machine-learning-models-part-1-99fde3b0cbc1

[#Blog1Part2]: Estimating Uncertainty in Machine Learning Models - Part 2 https://medium.com/comet-ml/estimating-uncertainty-in-machine-learning-models-part-2-8711c832cc15

[#Blog1Part3]: Estimating Uncertainty in Machine Learning Models - Part 3_ https://medium.com/comet-ml/estimating-uncertainty-in-machine-learning-models-part-3-796efba4a209

[#Blog2]: A Gentle Introduction to Uncertainty in Machine Learning https://machinelearningmastery.com/uncertainty-in-machine-learning/


[#Wilson2020]: Bayesian Deep Learning and a Probabilistic Perspective of Generalization https://arxiv.org/pdf/2002.08791.pdf


<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'none'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
