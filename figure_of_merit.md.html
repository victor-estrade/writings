<meta charset="utf-8" emacsmode="-*- markdown -*-">

**How to compute confidence interval**

In this document is explained how the confidence set is computed in our pipeline.
A 1D toy problem similar to the real HiggsML data illustrates the problem.

It is organized as follow :
1. In an ideal case (like at school)
2. First difficulty : with a likelihood using summary statistics
3. Second difficulty : with systematics (nuisance parameters)


In the ideal case
===============================================================================

This section describes how to compute a confidence set for a parameter from the observed data.


A simple example
-------------------------------------------------------------------------------

Let's start by how the data generative process is modeled.
Since this is a toy problem the model will perfectly match the generative process.

!!! tip
    In real life special care must be taken to check that the model match the data.
    Validating the model is out of scope of this document.

> Let's imagin that a machine is gathering leaves on the ground and also measures their length.
> An event is when the machine picked up a leaf and measured it.
> There is only two kind of leaves in the area, dividing the leaves into two groups noted S and B.

The data is a mixture of two distributions :
- a gamma distribution to represent the *background* events (group B)
- and a gaussian distribution to represent the *signal* events (group S)

Theory can predict the mixture coefficient of those two distributions.
The objective is to infer the signal mixture coefficient deviation, noted $\mu$, of the measured data from the theory.

The model describing the studied phenomenon can be fully described by giving its likelihood :
\begin{equation}
  L(\mu) = p(x|\mu) = \mu \times p(S) p(x|S) + p(B) p(x|B)
\end{equation}

where :
- $p(S)$ is the probability of a leaf to be in group S
- $p(x|S)$ is the likelihood of a measuring x knowing the leaf is in group S (gaussian distribution)
- $p(B)$ is the probability of a leaf to be in group B
- $p(x|B)$ is the likelihood of a measuring x knowing the leaf is in group B (gamma distribution)

It is supposed that the collection of each leaf is idependant and identically distributed ie. for a full dataset :
\begin{equation}
  L(\mu) = p(D|\mu) = \prod_{x \in D} \mu \times p(S) p(x|S) + p(B) p(x|B)
\end{equation}


Building a confidence set for $\mu$ requires two steps :
- A point estimator
- Reverting a statistical test using the previous point estimator



Step 1 : point estimation
-------------------------------------------------------------------------------

Point estimation in HEP is dominated by maximum likelihood estimator.

\begin{equation}
  \hat \mu = argmax_\mu L(\mu)
\end{equation}

In this simple case $L(\mu)$ is a closed form formula since all its components are known.


!!!
    Dans le manuscrit faire : cf annexe pur un exemple détaillé et mettre que le résultat final


Step 2 : Confidence set
-------------------------------------------------------------------------------

!!! error
    missing : the definition of confidence set.

!!! error
    homogénéiser les notations

This section is a bit technical but let's go !
Most of it is derived from [#Statbook].

For any function $t(X)$ derived from data $X$ with p.d.f $f(t|\theta)$ then :
\begin{equation}
 P(t_1 \leq t \leq t_2| \theta) = P[t_1(\theta) \leq t \leq t_2(\theta)] = \int_{t_1}^{t_2} f(t | \theta) dt
\end{equation}

Note that $[t_1,t_2]$ is written as an interval but in principle it could be any kind of domain in the t-space.
There is an infinite amount of such domain or interval $[t_1,t_2]$.
Usually one is looking for particular intervals.
The mosly used intervals are :
- central interval
- upper limit
- lower limit
- unified approach

What makes these intervals different from each other is the choice of an *ordering principle*.
The *ordering principle* is, as its name suggests, sorting the values of $t$
in order to choose how to grow the interval to reach the target probability.

The recommended ordering principle in HEP is the *unified approach* described in details in [#Feldman].
It is inspired by the Neyman-Pearson test which uses a likelihood ratio and is proven to be optimal for hypothesis testing.
This methods was firstly introduced to solve the issue of "flip-flopping" and empty intervals.
But also open the door for the profiled likelihood which deals with nuisance parameters as described later in this document.

---

J'aime pas cette equation elle donne l'impression qu'on se focalise sur la mesure x
\begin{equation}
 R(x) = \frac{P(x | \mu_0)}{P(x | \hat \mu)}
\end{equation}

On accumule les $\mu$ dans l'intervalle, en suivant la relation d'ordre donnée, jusqu'à ce que l'on atteigne la probabilité souhaité.
\begin{equation}
 ordering(\mu) = \frac{L(\mu)}{L(\hat \mu)}
\end{equation}

---

!!! tip
    TODO : comment on obtient cet interval en pratique ? En utilisant la méthode graphique (ln + 1/2)

**Notes :**

Glen's book page 131 :
> In fact, it can be shown that even if the likelihood function is not a Gaussian
> function of the parameters, the central confidence interval $[a, b] = [\theta - c, \theta + d]$
> can still be approximated by using
> $ log L (\theta^{+d}_{-c}) = log L_{max} - \frac{N^2}{2} $

- W.T. Eadie, D. Drijard, F.E. James, M. Roos and B. Sadoulet, Statistical Methods in Experimental Physics, North-Holland, Amsterdam (1971).
- A.G. Frodesen, O. Skjeggestad and H. T0fte, Probability and Statistics in Particle Physics, Universitetsforlaget, Oslo (1979).
Or better [#Statbook]



Untracktable sample-wise likelihood
===============================================================================

Intractable likelihood
-------------------------------------------------------------------------------

In our simple example the likelihood $L(\mu)$ is a closed form formula.
But in real experiment the likelihood includes many (millions) latent variables.
To compute the likelihood according to $\mu$ only one need to marginalize all the latent variables.
Meaning computing an integral in very high dimension which in intractable in practice.


Example Update : Poisson count process
-------------------------------------------------------------------------------

We keep the simple example but now we cannot use the closed formula of the likelihood.
This means that we need to find another way to find the maximum likelihood estimator and the interval estimator.

This would usually lead to an inverse problem.
But in our particular case there is another way around this issue.
Domain knowledge allows to produce another likelihood linking some observables to our parameter without latent variables.

In short, the process generating the event can be modeled by a rare Bernouilli distribution.
The problem is casted as a counting problem and counting rare Bernouilli event is very well approximated by a Poisson distribution.

The new observable is a summary statistic : the total number of events observed is assumed to follow a Poisson distribution.

\begin{equation}
  L(\mu) = p(n|\mu) = Poisson(n | \mu \gamma + \beta)
\end{equation}

!!!
    TODO : Simply show that we are back in buisiness using closed form formulas


Binning to improve
-------------------------------------------------------------------------------

!!!
    TODO : Put here what we know about how binning reduces the final incertainty



Numerical computing
-------------------------------------------------------------------------------

!!!
    TODO : Since the formulas cannot be sovled we are using numerical technics instead


Classifier to optimize binning
-------------------------------------------------------------------------------




Systematics and Nuisance parameters
===============================================================================


Definition
-------------------------------------------------------------------------------

!!!
    Explain what systematic effect and nuisance parameters are



Impact on maximum likelihood estimator
-------------------------------------------------------------------------------

!!!
    Makes it even less tracktable but also breaks classifier optimality



Profiled likelihood
-------------------------------------------------------------------------------

!!!
    The universal approach takes into account nuisance parameters using profiled likelihood ratio instead.




Biblio and links
===============================================================================




[#Statbook]: Statistical Methods in Experimental Physics 2nd edition

[#Feldman]: _A Unified Approach to the Classical Statistical Analysis of Small Signals_, https://arxiv.org/abs/physics/9711021




<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'none'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
