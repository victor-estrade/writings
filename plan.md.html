<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Plan 1.0**


Positioner le problème
===============================================================================


!!! error
    Ne pas se limiter à l'interval de confiance des physicien mais aussi inclure les méthodes ML de calcul de confiance
    cf Can you trust your model uncertainty et Evaluating predictive uncertainty challenge





Contexte
-------------------------------------------------------------------------------

- Explosion of ML in experimental science
- LHC at CERN
- HiggsML challenge



Statistical inference in ideal case
-------------------------------------------------------------------------------

!!!
    Here is introduced the statistical tools required and illustrated with a simple 1D example

- Introduce the 1D example
  - Measuring leaves length $x$ (for whatever reason) with a ruler
  - 2 groups of leaves :
    - 1 following a Gamma distrib (group B)
    - and another longer following a Gaussian distrib (group S)
  - Want to know the number of leaves in group S
  - Want to know the deviation of the $n_S$ from prediction (for numerical convenience)
  - Giving the likelihood of the data : [formula]
- Maximum likelihood point estimator
  - Easy with maximum likelihood !
- Invert statistical test to get a confidence set
  - The final objective is a confidence set (or interval)
  - We get it by inverting a statistical test.



Inverse problem
-------------------------------------------------------------------------------

!!!
    The first difficulty : Not a tracktable likelihood at sample level

- No likelihood at the sample level
  - Now the likelihood for $x$ is intractable in many interesting case
  - (which is the case fot the HEP experiments)
  - Because there is a very large number of latent variables
  - But it is possible to sample from a simulator
- Inverse problem
  - This leads to an inverse problem (inverting a simulator/sampler)
- Particular inverse problem -> looking for a mixture coefficient
- (or not) likelihood at the distribution level
  - But in our particular case an approximative model is available
  - Poisson count process as an approximation of counting iid Bernoulli rare process
  - The simulator is part of the model through $\gamma$ and $\beta$

!!!
    binning will be introduced later whith the classifier improvement.
    We now introduce systematics because it is more important.


Systematics
-------------------------------------------------------------------------------

!!!
    The second difficulty : systematic effect of the nuisance paramters


- What are systematics ?
  - Nuisance parameters are parameters required to fully describes the data generating process but not of direct interest
  - Systematic effect is the effect of nuisance parameters on the data
  - Update the 1D exampl :
    - Systematics = a small offset or scale factor on the ruler
      - (we go with the scale because it will be usefull later)
      - impossible to disentangle -> measured x = offset + true value or scale * true value
    - Objective = something related with the mean length of the leaves
    - Increases uncertainty on the final measurement
  - Calibration is required to partially disentangle the nuisance parameters and the parameter of interest
    - It is an additionnal information -> an additional measurement -> an additional likelihood
- Systematics always increases the confidence set volume
- Profiled likelihood method to take systematics into account in uncertainty measurement (volume of confidence set)


Classifier to improve binning
-------------------------------------------------------------------------------

- Binning is better !
  - A good idea to improve the inference is to do binning as long as the Poisson approx remains valid
  - show math proving that 2 bins is better no matter what threshold is used
  - show math proving that more bins is probably better too
  - best to have pure signal or pure background bin
- High dimensional data histogram is too expensive
  - Multi dimensional data usually cary more information
  - Unfortunately binning scales exponentially with increase of dimension
- dimension reduction
  - Need to find a good dimension reduction keeping most information about our parameter of interest
  - Can be crafted by hand but it requires domain knowledge and many work hours
  - hence the idea of using Machine learning
- Figure of merit is not a tracktable (additive) quantity (cannot use as a loss)
  - So we can use a proxy loss to train our ML model
  - Recall that separating S and B events is reduces the uncertainty -> classifiers
- Bayes optimal classifiers gives sufficient summary stat (no systematics setting)
- Systematics breaks Bayes optimal classifier garanty


Solutions and outline
-------------------------------------------------------------------------------

- Regularization to train a robust classifier
- Change cross-entropy for a lower bound of final uncertainty
- Direct regression (likelihood free setting)
- Mining data for nuisance parameters to improve model



!!! error
    HiggsML data est intro section 1 mais on en parle plus du tout après....

!!! error
    Le problème de comptage n'est pas assez mis en avant


Regularization
===============================================================================

!!! error
    Reprendre les papiers pour compléter

Add regularization to cross entropy loss
-------------------------------------------------------------------------------

- If the classifier output is independant from the nuisance parameters then we win !
  - If it is just less dependant it will still decrease the final uncertainty.



Data augmentation
-------------------------------------------------------------------------------


- Avantages / drawbacks summary



Tangent Propagation
-------------------------------------------------------------------------------

- Avantages / drawbacks summary


Pivotal neural networks
-------------------------------------------------------------------------------

- Related to the DANN but without unlabeled data
- Avantages / drawbacks summary



Performances ?
-------------------------------------------------------------------------------




Tackling the distribution
===============================================================================



Neural stat & inferno
-------------------------------------------------------------------------------

- Training neural networks to produced summary statistics
  - Permutation invariant architectures
  - Losses on summary stats (and on samples for neural stat)
- Training on a lower bound of the final uncetrainty
  - Cramer Rao bound
- Limitations
  - Differentiable simulator
  - Reverse a large hessian matrix can be complicated




Performances on toys ?
-------------------------------------------------------------------------------



Direct regression
-------------------------------------------------------------------------------

- Motivation : likelihood free, no differentiable simulator, simpler
- Measuring uncertainty with Density networks
  - There is better ways nowadays
- Training details
  - The dynamic of training is unusual
  - Adjust Adam parameters to reduce intertia
- Include samples weights into the architecture
- Two ways to handle nuisance parameters
  - Ingore them (harder on the network)
  - Profile them (give as input and marginalize)


More details
-------------------------------------------------------------------------------



Performances ?
-------------------------------------------------------------------------------



Improve model / extract more info from data
===============================================================================

n_i are not sufficient
-------------------------------------------------------------------------------

- $n_i$ are not sufficient for $\alpha$



Extract using a regressor
-------------------------------------------------------------------------------




Discussion and conclusion
===============================================================================





Lost and found
===============================================================================

- les résultas des XP
- Higgs re-simulator
- 1er Travaux sur sigma mu

- How it is done
  - hand crafted dimension reduction (ie ask a theorist to find a feature) is not always possible and requires a lot work hours
  - Poisson count process & binned Poisson count process
  - Although the model seems simple, most of it is in the simulator ie in the estimation of $\gamma$ and $\beta$
- Other methods exists but do not fit perfectly to our problem
  - ABC is likelihood free -> we have a likelihood
  - Variational inference -> is known to underestimate the uncertainty, and is looking for p(x) instead of p(mu|x)
  - Bayesian networks -> Other have tried ... need to check more in depth
  - Mining gold -> I do not have access to the simulator (proprietary code)





<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'none'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
