<meta charset="utf-8" emacsmode="-*- markdown -*-">



Alternatives
===============================================================================


Inferno
-------------------------------------------------------------------------------

La méthode INFERNO combine à la fois la réduction de dimension et la construction d'un histogramme.



L'estimation directe
===============================================================================

!!! WARNING
    TODO : Pour le moment j'ai exclu ce pipeline de ce document et des résultats expérimentaux.

Core idea
-------------------------------------------------------------------------------

The parameter of interest impact is not on the observables $x$ but on the distribution of the observables $p(x|\mu)$.
This is invisible at the sample scale.
With only one sample $x$ it is not possible to get knowledge on the parameter $\mu$.
Measuring properties of the distribution is only moving the problem from a sample scale to a dataset scale.

The learning problem is to get from an empirical distribution $D = \{x_i\}_{i=0}^N \in R^{d\times N}$ to a real number $\mu \in R$.
This is a regression problem.


The training and testing datasets can be obtained by sampling the parameter from a prior distribution then using the simulator to transform this parameter to its corresponding dataset.

The last detail to take care of is the size of the training datasets.
The ideal size is the expected size of the experimental data or a range of possible data size.
But it may be too large to train a neural network with available computation power in a reasonable time.
The size of the training datasets is then a tradeoff between the size of the experimental dataset and available computation power.


Density networks
-------------------------------------------------------------------------------


A way of approximating the posterior $p(\mu | x)$ is to define a tractable but flexible enough family of distribution $q_\phi(\mu | x)$ parametrized by $\phi$.

Initially introduced in [Bishop94mixturedensity] as a generalization of least square methods to train neural network, mixture density networks (MDN) can be made as powerful as required while staying tractable.
Used as a regressor MDN allows to estimate both the expected value and its uncertainty as detailed is the next section.

The target density is approximated by a linear combination of kernels $k$ :

\begin{equation}
    q_\phi(\mu | x) = \sum_{i=0}^K m_i(x ; \phi) k_i(\mu | x ; \phi)
\end{equation}
where $m_i(x ; \phi)$ are the mixture coefficient
and the kernels $k_i(\mu | x ; \phi)$ usually chosen as Gaussian :
\begin{equation}
    k_i(\mu | x ; \phi) = \frac{1}{\sigma_i(x ; \phi) \sqrt{2 \pi}} e^{- \frac{1}{2} \left ( \frac{\mu-y_i(x ; \phi)}{\sigma_i(x ; \phi)} \right )^2}
\end{equation}

$m_i(x ; \phi)$, $\sigma_i(x ; \phi)$ and $y_i(x ; \phi)$ are the outputs of a neural network, whose parameters are gathered in $\phi$, and taking the data as input.
Finally, the mixture coefficient have to sum up to 1.
\begin{equation}
    \sum_{i=0}^K m_i(x ; \phi) =  1
\end{equation}

The motivation for this approximation is twofold.
First, given enough well chosen parameters a Gaussian mixture model can approximate any density.
Second, a neural network with enough hidden unit is able to approximate any continuous function with arbitrarily precision.
Combining these two properties leads to an arbitrarily powerful approximation of any conditional density $p(\mu|x)$ given enough resources.


Training density networks
-------------------------------------------------------------------------------


\begin{equation}
    \phi^\star = argmax_\phi \mathcal L (\phi)
\end{equation}
\begin{equation}
    \mathcal L (\phi) = \sum_{i=0}^K m_i(x ; \phi) k_i(\mu | x ; \phi)
\end{equation}


For convenience the optimization is usually turned into a minimization of the negative log likelihood (NLL) :
\begin{equation}
    \phi^\star = argmin_\phi - \log \mathcal L (\phi)
\end{equation}

In the simple case of one Gaussian component ($K=1$) the NLL is :
\begin{equation}
    \phi^\star = argmin_\phi \left\{ \log(\sigma(x;\phi)) + \frac{1}{2}\log(2\pi) + \frac{(\mu^\star - y(x;\phi))^2}{2\sigma(x;\phi)^2} \right\}
\end{equation}

Similarly to training a regular neural network regressor with least square, MDN's training is supervised.
Therefore requires data for which the ground truth is available which is verified in our case since we need both observed data $x$ and the associated value for  $\mu^\star$.

Finally the likelihood, with Gaussian kernels, is fully differentiable making possible the use of stochastic gradient descent methods to obtain the neural network parameters $\phi$.


Datasetwise input
-------------------------------------------------------------------------------

In order to accurately capture the complex mapping between the data and the parameters the architecture of the neural network should embody the constraints of the chosen family distribution.

If the studied process is stochastic the observations are usually composed of repeated independent measurements of an event.
Then the observable is not a real valued vector $x \in R^d$ but a set of data points $D = \{x_i \in R^d \}_{i=0}^N$.
The output is still a real valued vector meaning that the neural network architecture should include some reduction function to map the set of vector $D$ to a single vector $y$.
Usual candidates are averages, minima, maxima, products, sums, geometric means or others that reduce the dimension and remain invariant to the input order of the input vectors.
In practice the average is the favorite one [needcite].

Section 7 of [lucas:hal-01791126] gives a proof that the given *"permutation invariant neural networks"* architecture is a universal approximator of permutation invariant functions.

The next figure summaries **one layer** of such neural network.

![Figure [fig:average_layer]:
  Single layer of the datasetwise neural network. $U$ and $V$ are 2 matrices of the same shape. $r(X)$ is the result of the reduction of the input data $X$. In practice the + operator is relying on broadcasting to sum $V.r(X)$ or $b$ to each sample in $U.X$.
](./figs/average_layer.png)


Weighted input
-------------------------------------------------------------------------------


When the studied process includes some very rare events the simulation uses importance sampling \needcite.
The simulation output includes importance weights to allow many rare events to be produced while keeping the distribution of events similar to reality.
The neural network must take into account the importance weights to accurately regress the parameters.
Which leads to use weighted average instead of simple average for example and makes maximum and other unweighted reduction ill-suited to this setting.

More precisely, for 2 samplesets $D$ and $D^\prime$ if the associated empirical distribution are equal then the neural network output should also be equal.

\begin{equation}
    \forall x, p_D(x) = p_{D^\prime}(x) \implies f(D; \phi) = f(D^\prime; \phi)
\end{equation}

with,
\begin{equation}
    p_D(x) = \sum_{v \in D} w_i \delta (x - v)
\end{equation}
and $\delta$ is the Dirac distribution function.


![Figure [fig:average_layer]:
  Single layer of the weighted datasetwise neural network. The reduction function takes into account the sample weights.
](./figs/weighted_average_layer.png)


Ignore nuisance parameters
-------------------------------------------------------------------------------

The first way is to simply ignore them and directly approximate the marginal posterior $p(\mu|D)$.
This method is refered to as **Blind Regressor**.
The training dataset should include a representative set of samplesets for the problem.
Meaning that instead of simply sampling a new value $\mu_i$ for each sampleset $D_i$ also sample the nuisance parameters from theirs prior distribution to feed them to the simulator $D_i = G(\mu_i, \alpha_i)$.



Marginalizing nuisance parameters
-------------------------------------------------------------------------------

The other way is to learn the posterior $p(\mu | D, \alpha)$ then to marginalize the nuisance parameters.
\begin{equation}
    p(\mu | D) = \int d\alpha ~ p(\alpha | D) ~ p(\mu | D, \alpha)
\end{equation}

This is inspired by [Baldi_2016].
This method is refered to as **Parametrized Regressor**.

!!! Tip
    Expliquer que $\alpha$ est en entré du NN

This integral can be approximated with Monte Carlo.

\begin{equation}
  \int \int d\alpha ~ p(\alpha | D) ~ p(\mu | D, \alpha)
  \approx \sum_i w_i ~ p(\mu | D, \alpha_i)
\end{equation}

Where $p(\mu | D, \alpha)$ is approximated using a trained MDN as seen in the previous sections.
In the case of a MDN $f$ with gaussian kernels, $f$ produces the mixture parameters $m_i, y_i, \sigma_i$ from the experimental data $D^\star$ and sampled $\alpha$.







Le pipeline hybride
===============================================================================

Idée générale
-------------------------------------------------------------------------------

Le pipeline classique utilise un classifieur comme réduction de dimension.
Ce choix est motivé par la capacité du classifieur à garder au maximum l'information sur le paramètre d'intérêt.
Sauf que l'étape de maximisation de likelihood traite $\mu$ et $\alpha$ de la même manière.
La perte d'information sur $\alpha$ peut dégrader la qualité du maximum likelihood.

On va donc chercher à extraire une nouvelle information sur $\alpha$ depuis les données expérimentales.

Pour ce faire on utilise un regresseur sur les données expérimentales.



Mise à jour du modèle statistique
-------------------------------------------------------------------------------

La différence est l'intégration d'une nouvelle mesure sur les données expérimentales s'ajoutant à la formule de la NLL de la même manière que la calibration.

!!! WARNING
    TODO : expliquer mieux !


\begin{equation}
  L(\mu, \alpha) =  Poisson(X | \mu, \alpha) \times Calib(Other measure | \alpha) \times N(\alpha | k )
\end{equation}


!!! INFO
    Validité de cette opération statistique ?  X et k ne sont pas indépendant...

![Figure [fig:value_loop_hybrid]:
  Zoom on the negative log likelihood value module using hybrid pipeline
](./figs/modules/value_loop_hybrid.png height="700px")


![Figure [fig:regressor]:
  Regressor
](./figs/modules/regressor.png)






<!-- <link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?"> -->
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
