<meta charset="utf-8" emacsmode="-*- markdown -*-">


Évaluation empirique
===============================================================================


Critères de performance
-------------------------------------------------------------------------------

Les diverses alternatives seront évaluées selon 2 critères :
1. La Mean Squared Error (MSE) de l'estimateur de $\mu$
2. La taille de l'intervalle de confiance

La MSE combine à la fois le biais et la variance de l'estimateur qui sont primordiaux pour une mesure de qualité.
Un estimateur biaisé est un estimateur quasiment inutilisable en Physique (source ?).
De plus une grande variance implique une estimation peu stable donc assez peu satisfaisante.

La MSE est donc la première mesure de performance pour sélectionner les hyper-paramètres des algorithmes de ML impliqué.

La taille de l'intervalle de confiance est précisément ce que l'on souhaite réduire au maximum.
Il représente l'incertitude finale sur le paramètre de nuisance.


Mesure de performance
-------------------------------------------------------------------------------

Afin de mesurer la MSE des estimateurs le processus complet (entraînement et estimation) est répété 30 fois en changeant uniquement la seed.
La seed contrôle à la fois l'échantillonnage du simulateur mais aussi l'initialisation de réseaux de neurones.
On échantillonne ainsi 30 valeurs de $\hat \mu$ et 30 intervalles.

Ces échantillons dépendent des valeurs "vraies" $\mu^\star$ et $\alpha^\star$ choisies.
Afin d'observer les performances en dehors du nominal on utilise 3 valeurs de $\alpha^\star$ : le nominal plus ou moins 1 déviation standard telle que définie par la calibration.
Plusieurs valeurs de $\mu^\star$ sont utilisés :
-  HiggsML [0.5, 1, 1.5, 2]
-  Toy 1D [0.5, 0.75, 1, 1.25, 1.5]
-  Toy 3D [0.5, 1.25, 2]

!!! ERROR
    Et merde ! Le nominal est pas inclus dans le toy 3D.
    À corriger.

Au final plusieurs dizaines d'estimation de $\hat \mu$ et de son intervalle de confiance sont réalisées afin de mesurer empiriquement la MSE et la taille moyenne de l'intervalle de confiance en fonction de $\mu^\star$ et $\alpha^\star$.





Estimateur de mu et de alpha
-------------------------------------------------------------------------------

Lors de la maximisation de la likelihood $\mu$ et $\alpha$ sont traités exactement de la même manière.
C'est uniquement à l'étape de la construction de l'intervalle de confiance ou de l'entraînement des réductions de dimension que la différence entre paramètre d'intérêt et de nuisance intervient.

Les divers estimateurs de $\mu$ et $\alpha$ sont caractérisés par 2 éléments qui définissent la manière de calculer la NLL :
- Le pipeline utilisé
    - idéal
    - classique
    - régression directe
    - hybride
- La réduction de dimension utilisée
    - classifieur classique : Gradient boosting
    - classifieur classique : Neural network
    - data augmentation
    - tangent propagation
    - Pivotal neural network
    - feature filter
    - inferno

Le pipeline idéal ou la régression directe n'emploient pas de réduction de dimension donc pas de classifieurs ou d'inferno.


Estimateurs de l'intervalle de mu
-------------------------------------------------------------------------------

Identiquement à l'estimation des paramètres du modèle statistique les différences sur l'estimation de l'intervalle de confiance proviennent de la façon de calculer la NLL.



Illustration des résultats
-------------------------------------------------------------------------------

!!! WARNING
    TODO : Quelles tableaux/graphiques pour illustrer mes claims et comment ils sont obtenus

Les performances mesurées sont la MSE et la taille moyenne de l'intervalle de confiance à $\mu^\star$ et $\alpha^\star$ fixé.
Afin de comparer les diverses méthodes on plot sous forme de boite à moustache les critères de performances pour chaque méthode.
Une boite resserrée indique une méthode stable vis-a-vis des variations de $\mu^\star$ et $\alpha^\star$.
La hauteur de la boite (en particulier la médiane) indique la qualité de la méthode (pour les deux critères : lower is better).

Des graphes remplaçant les boites à moustaches par moyenne/variance sont aussi disponibles mais moins informatifs (trop sensibles aux valeurs extrêmes à mon goût).



<!-- <link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?"> -->
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
