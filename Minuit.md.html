<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Minuit**



Intro
-------------------------------------------------------------------------------


Suite à la réunion de ce matin (12/04/2021) j'ai cherché à comprendre + en profondeur l'algo MIGRAD de Minuit afin de trouver comment choisir la tolerance.

En lisant la doc de MINUIT et les papiers fondateurs j'ai pu trouver que :
- L'algo MIGRAD était + connu sous le nom BFGS qui est l'état de l'art des méthodes Quasi-Newtonienne pour la minimization
- Que la tolérance : tout le monde s'en fous ! (ie. le diable est dans les détails -> je l'ai trouvé)
- EDM (estimated distance to minimum) n'est décrit que dans le papier d'origine qui est derrière un paywall
- Mais en fait c'est + ou - (à la covariance près) juste la norme du gradient

Ce qui fait du sens en fait. On a un algo qui cherche numériquement à diminuer le gradient à zéro.
Comme le zéro n'est pas vraiment atteignable on doit choisir une valeur acceptable.



MIGRAD's algorithm
===============================================================================


The program
-------------------------------------------------------------------------------



BFGS
-------------------------------------------------------------------------------

Minuit's default algorithm is MIGRAD.
This algorithm is described in [#Fletcher70].

In the documentation the minimization algorithm is refered as a "variable metric method".
Googling it does not gives much information with these key words.

Today we mostly use the term *Quasi-Newton* to designate these methods.

MIGRAD is an implementation of the BFGS (Broyden-Fletcher-Goldfarb-Shanno) algorithm.



Estimated distance to minimum
-------------------------------------------------------------------------------


[#Minuit75] describes how the Estimated Distance to the Minimum is computed :

![Figure [fig:EDM]:
  Exctract of [#Minuit75] page 8
](./Minuit/EDM_minuit_paper.png)

\begin{equation}
EDM = \nabla f^T V \nabla f
\end{equation}

with $\nabla f$ the gradient and $V$ the covariance matrix.

Basically it is the norm of the gradient.
Which makes sense since the algorithm is numerically decreasing the gradient toward zero.



Tolerance
-------------------------------------------------------------------------------


The tolerance controls the convergence criterion.
The convergence criterion is simply : the value of the (estimated or real) gradient is below the tolerance (usually the default is $10^{-5}$).

In books and documentation about BFGS the tolerance (usually noted $\epsilon$) is left to the user.
The devil is in the details and I think I found the detail.
There is **no prescription to how the tolerance should be set**.



Algorithm very detailed description
-------------------------------------------------------------------------------

[https://www.osti.gov/servlets/purl/4252678](https://www.osti.gov/servlets/purl/4252678)





Mail
===============================================================================


"paramètres" désigne les paramètres que Minuit cherche pour maximiser la Likelihood ie. mu et alpha
"Variable Metric Method (VMM)" est plus couramment appelé Méthode Quasi-newtonienne de nos jours.

L'algo de descente utilisé par Minuit est connu sous l'appellation BFGS de nos jours.


La doc de la version FORTRAN (1994)
-------------------------------------------------------------------------------

Lien : https://root.cern.ch/download/minuit.pdf

Page 8 section 1.2.1 : La formule pour les transformations en interne pour appliquer des limites aux paramètres
Pour nous mu est alpha doivent être positif. La formule qu'on utilise n'est donc pas celle là.

Page 14 section 2.4 : Avertissement sur la précision des calculs avec float 32.

Page 26 en bas : MIGraD formule indiquant le lien entre la tolérance de l'algo BFGS et celle donnée à Minuit (tolérance = 0.001 * tol * UP)
UP vaut 0.5 dans notre cas (Likelihood Fit à 1 sigma)

Page 28 SET ERRordef : indication que ERRORDEF = 0.5 si la FCN est un log likelihood

Page 36 section 5.7 : Un autre avertissement concernant la précision

Page 43 Chapter 7 : Comment interpréter les valeurs "d'erreur" donnée par Minuit et les détails sur ERRORDEF.



La doc de la version C++
-------------------------------------------------------------------------------

Lien : https://iminuit.readthedocs.io/en/stable/_downloads/165972a3b917b0f71940b3aac68fb978/mnusersguide.pdf


Page 13 section 1.3.1 : La formule pour les transformations en interne pour appliquer des limites aux paramètres
Pour nous mu est alpha doivent être positif


Page 15 section 1.5.1 :  Explication sur ERRORDEF et le lien avec l'interval de confiance (1-sigma)

Page 23 section 2.7 :  Avertissement concernant la précision float32

Page 45 Chap 5 : toutes les recettes de cuisine pour faire marcher Minuit.




Concernant la doc de iMinuit sur le net :
-------------------------------------------------------------------------------

La doc sur la précision est ici :
https://iminuit.readthedocs.io/en/stable/reference.html?highlight=precision#iminuit.Minuit.precision

La doc sur l'algo migrad est là
https://iminuit.readthedocs.io/en/stable/reference.html?highlight=precision#iminuit.Minuit.migrad
Et renvoi vers un papier qui est bloqué par un paywall.
Du coup je le join au mail




Le papier original de Minuit 1975 (join au mail)
-------------------------------------------------------------------------------

Page 10 Subroutine MIGRAD : explique comment marche l'algo de descente de Minuit (avant qu'il ne soit renommé BFGS je suppose)
En particulier page 11 on a le critère de convergence et la formule pour calculer EDM


C'est dans ce papier que j'ai trouvé la ref vers :
https://www.semanticscholar.org/paper/A-New-Approach-to-Variable-Metric-Algorithms-Fletcher/bb787c61efda995b99fb939b743c99cda3f0b9f4
Qui est + ou - le papier original de l'algo de descente (détaillé Section 5 page 4)

L'algo a probablement été un peu amélioré depuis.

Je n'ai pas trouvé l'explication sur le cas où Minuit s'autorise de changer le critère de convergence à 10 * tolérance au lieu de tolérance.



Le livre
-------------------------------------------------------------------------------


Ce livre est cité de temps en temps : https://cds.cern.ch/record/1019859/files/9789812567956_TOC.pdf
Le chapitre 8 et 9 donnent beaucoup de détails sur ce qu'on est en train de faire.




References
-------------------------------------------------------------------------------


[#StatMethods]: Statistical methods in experimental science 2nd Edition

[#Fletcher70]: R. Fletcher. A new approach to variable metric algorithms. Comput. J.13, 317 (1970) https://academic.oup.com/comjnl/article/13/3/317/345520

[#Minuit75]: MINUIT a system for function minimization and analysis of the parameter errors and correlations


<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
