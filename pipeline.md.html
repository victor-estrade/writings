<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Pipeline description**

Documentation of the pipeline of producing inference using pseudo code.
Inference is two-fold : *point estimation* and *confidence interval estimation*.

The objective of the document is to explain the inference pipeline first from a theoretical point of view and then the modifications to enable experiments and describe possible improvements.
The direct regression method will be described in another document.

- The 1st section gives generalities
- The 2nd section describes the theoretical pipeline
- The 3rd section describes the differences with my implementation
- The 4th section describes systematic aware pipeline differences
- The 5th section describes inferno pipeline differences



General
===============================================================================

Describes general features shared by all methods.

Simulator
-------------------------------------------------------------------------------

The **simulator** is a program allowing to sample a virtually infinite amount of data points $x$ given some parameters $\mu$ and $\alpha$.

The simulator produces:
- the *observables* as a matrix `X`
- the *labels* as an array `y`
- the sample (importance) *weights* `w`
Usually those 3 elements will be gathered into one object (tuple) named `Data`.

Usage (usually the number of samples will be discarded) :
```python
mu = 1
alpha = 1
Data <- simulator(mu, alpha, n_samples=5000)
#  OR
X, y, w <- simulator(mu, alpha, n_samples=5000)
```

Notations / Glossary
-------------------------------------------------------------------------------

- $\mu$ : the parameter of interest (in general)
- $\alpha$ : the nuisance parameter (in general)
- $\mu^\star$ : the true unknown value of the parameter of interest that generated experimental data
- $\alpha^\star$ : the true unknown value of the nuisance parameter that generated experimental data
- $\hat \mu$ : the estimator of the parameter of interest
- $\hat \alpha$ : the estimator of the nuisance parameter
- $\gamma$ : the expected number of signal events
- $\beta$ : the expected number of background events
- $\gamma_i$ : the expected number of signal events in bin number $i$
- $\beta_i$ : the expected number of background events in bin number $i$





Standard Classifier method
===============================================================================

How to infer using a classifier.



Training the classifier
-------------------------------------------------------------------------------

The classifier is trained on a simulated dataset generated with nominal values ie. with the expected or most probable values of $\mu$ and $\alpha$.
Moreover the importance weights of signal and backgrounds are rebalanced (overrides the given value of $\mu$) to avoid issues with heavily imbalanced class.


```python
mu_nominal = 1
alpha_nominal = 1
Data_train_nominal <- simulator(mu_nominal, alpha_nominal)
classifier <- SomeAlgo.fit(Data_train)
```


The log likelihood
-------------------------------------------------------------------------------

The model used here is a Poisson count process in $K$ independent bins indexed by $i$.

\begin{equation}
L(n | \mu, \alpha) = \prod_i Poisson(n_i | \mu \times \gamma_i(\alpha) + \beta_i(\alpha))
\end{equation}
\begin{equation}
L(n | \mu, \alpha) = \prod_i \frac{(\mu \times \gamma_i(\alpha) + \beta_i(\alpha))^{n_i}}{n_i!} e^{ - \mu \times \gamma_i(\alpha) - \beta_i(\alpha)}
\end{equation}

\begin{equation}
\log L(n | \mu, \alpha) = \sum_i n_i \times \log (\mu \times \gamma_i(\alpha) + \beta_i(\alpha)) - \log (n_i!) - \mu \times \gamma_i(\alpha) - \beta_i(\alpha)
\end{equation}

!!! note
    As we can see : $\gamma_i$ and $\beta_i$ are required to compute the likelihood.
    The details about computing $\gamma_i$ and $\beta_i$ is given in the next section.

In addition to the Poisson count process the calibration experiment updates our knowledge about the nuisance parameters $\alpha$.
These other measurements `Calib` are taken into account in the likelihood.
The calibration is designed to be independent of the experiment and to depend only on the nuisance parameter $\alpha$.

\begin{equation}
L(n, Calib | \mu, \alpha) = L(n | \mu, \alpha) \times L(Calib | \alpha)
\end{equation}

Gives :

\begin{equation}
L(n, Calib | \mu, \alpha) = \prod_i Poisson(n_i | \mu \times \gamma_i(\alpha) + \beta_i(\alpha)) \times L_{Calib}
\end{equation}

Very often $L_{Calib}$ is reported as a gaussian.
The parameter (mean $\alpha_c$ and std $\sigma_c$) of the gaussian are determined by the calibration measurements `Calib` (ommited here to ease reading).

\begin{equation}
L_{Calib} = Normal(\alpha_c, \sigma_c| \alpha) = \frac{1}{\sigma_c\sqrt{2 \pi}} e^{\frac{(\alpha - \alpha_c)^2}{2 \sigma_c^2}}
\end{equation}

Finally gathering it all :

\begin{equation}
\log L(n | \mu, \alpha) = \left [ \sum_i n_i \times \log (\mu \times \gamma_i(\alpha) + \beta_i(\alpha)) - \log (n_i!) - \mu \times \gamma_i(\alpha) - \beta_i(\alpha) \right ]
    - log(\sigma_c) - log(\sqrt{2 \pi}) + \frac{(\alpha - \alpha_c)^2}{2 \sigma_c^2}
\end{equation}


The negative log likelihood is computed via the `formula` function :
```python
def formula(mu, alpha, gamma_i, beta_i, n_i):
  global alpha_c, sigma_c
  nll = poisson_nll( mu * gamma_i + beta_i, n_i ) + gaussian_nll(alpha, alpha_c, sigma_c)
  return nll
```


!!! note
    The calibration (how to find $\alpha_c$ and $\sigma_c$) is explained somewhere else.

!!!
    Since the measurements may not be very informative about $\alpha$,
    the calibration is also very helpful to avoid plateau around the maximum likelihood or local maxima.



Measuring the negative log likelihood on one point $(\mu, \alpha)$
-------------------------------------------------------------------------------

To perform a numerical minimization of the negative log likelihood the first step is to write a function computing it.

As seen previously we need $\gamma_i$ and $\beta_i$ to compute the likelihood.
$\gamma_i$ and $\beta_i$ are estimated using the Simulator.
The simulator allows to separate the signals and the backgrounds since labels are available.

```python
X_test <- measurement on real experiment
decision_test = classifier.predict_score(X_test)
n_array     = bining(decision_test)

def compute_nll(mu, alpha):
    global classifer  #from previous training
    global n_array

    Data_valid  <- simulator(mu, alpha)
    signal_valid <- signal_only(Data_valid)
    background_valid <- background_only(Data_valid)

    decision_signal = classifier.predict_score(signal_valid)
    decision_background = classifier.predict_score(background_valid)

    gamma_array = bining(decision_signal)
    beta_array  = bining(decision_background)

    nll = formula(mu, alpha, gamma_array, beta_array, n_array)
    return nll
```

Unfortunately to compute the NLL of a candidate value $(\mu, \alpha)$ one has to rerun the simulation.
Of course the full simulation is not rerun.
Many nuisance parameters influence starts at a late stage of the simulation process so only this last part requires to be recomputed.


Point estimation
-------------------------------------------------------------------------------

We want to get $(\hat \mu, \hat \alpha)$ the maximum likelihood estimator for $L$.
If a analytical formulas cannot be derived from the likelihood a numerical approach is the way to go.

Basically any minimizer can do the jobs.
**Minuit** is the standard for negative log likelihood minimization in HEP.
It assumes that the given function is smooth [citation ?] and allow only one minimum which is very often the case in physics settings [citation ?].

!!! Tip
    I have not checked in details how Minuit works yet.

```python
# Point estimation
mu_mle, alpha_mle = Minuit.mingrad(compute_nll)
```

Here is a sketch for a gradient descent algorithm.
```python
# exemple de minimisation
start_mu = 1
start_alpha = 1

current_mu = start_mu
current_alpha = start_alpha
while not convergence:
    left_value = compute_nll(current_mu - epsilon, current_alpha - epsilon)
    right_value = compute_nll(current_mu + epsilon, current_alpha + epsilon)
    gradient = (right_value - left_value) / (2 * epsilon)
    step = ...
    next_mu = current_mu - step
    next_alpha = current_alpha - step
    convergence = is_convergence(...)
```
What I want to emphasize here is that `compute_nll` may be called many times before convergence.
Unfortunatelly memoizing is not possible because the candidate values of $(\mu, \alpha)$ are different for each call.

Moreover gradient descent methods requires to estimate local gradient (and hessian) with scales teribly with increasing the number of parameters.





Confidence interval
-------------------------------------------------------------------------------

Confidence interval are computed by inverting a hypothesis test.

$H_0(\mu = \hat \mu)$ and $H_1(\mu \neq \hat \mu)$.

A likelihood ratio test is conduct and the acceptance region is taken as the confidence interval.

A standard likelihood ratio lokks like :
\begin{equation}
\lambda =  \frac{L(Data | \mu \neq \hat \mu)}{L(Data | \mu = \hat \mu)}
\end{equation}

But to get rid of the nuisance parameter $\alpha$ there is 2 options :
1. Marginalization
2. Profiled likelihood

The second is prefered in our case for yet-to-discovered reasons.

!!! Tip
    I could not find why marginalization is not even considered in the papers I read.

A profiled likelihood test is conduct "and inverted" to compute the confidence interval of $\star \mu$.

\begin{equation}
\lambda(\mu, Data) =  \frac{\sup_{\alpha} L(Data | \mu \neq \hat \mu, \alpha)}{\sup_{\mu, \alpha} L(Data | \mu, \alpha)}
\end{equation}
\begin{equation}
\lambda(\mu, Data) =  \frac{\sup_{\alpha} L(Data | \mu \neq \hat \mu, \alpha)}{L(Data | \hat \mu, \hat \alpha)}
\end{equation}




Confidence interval estimation
-------------------------------------------------------------------------------

The previous method works fine when analytical formulas are available.
But in our case closed form formulas are not available.

According to **Wilks theorem** :
the distribution of $âˆ’2 \log [ \lambda (\mu, Data) ]$ is approximately a $\chi^2_p$ with $p$ degrees of freedom, $p$ being the dimension of $\mu$.


!!! WARNING
    TODO : Explain from here the ln + 1/2 method to estimate the confidence interval

!!! WARNING
    TODO : find again why this is supposed to cover.




Gathering it all
-------------------------------------------------------------------------------

Here are the steps :
1. Train a Classifier
2. Choose good starting points for NLL minimization
3. Use `Minuit.mingrad` to estimate the maximum likelihood
  * numerous calls of the `classifier.decision` and `simulator`
4. Use `Minuit.minos` to estimate the confidence interval
  * even more calls of the `classifier.decision` and `simulator`

Alternative to `Minuit.minos` is to use the hessian to estimate the confidence interval.
Cheaper but symetric, less precise, assumes hyperbolic/parabolic NLL.



Differences with my implementations
===============================================================================


Simulated Test set
-------------------------------------------------------------------------------

Since the test set from real experiment is not available we use the simulator to produce the test set.
The hidden values of the input parameters are noted $\mu^\star$ and $\alpha^\star$.

```python
mu_star = 1.1  # for example
alpha_star = 1.03  # for example
Data_test <- simulator(mu_star, alpha_star)
```

The objective is to measure how well the various models manage to find $\mu^\star$ and $\alpha^\star$.



Hessian confidence interval
-------------------------------------------------------------------------------

Instead of the ln + 1/2 method with profile likelihood we use the Hessian as an approximation.
This gives a symetric interval assuming that the negative log likelihood is parabolic/hyperbolic around the minimum (gaussian-like approximation).

!!! WARNING
    TODO : check if hyperbolic or parabolic ? And double check on the gaussian approx



Systematic aware classfiers
===============================================================================

This section describes Data augmentation, Tangent propagation, Pivot.

Differences with the Standard classifier pipeline
-------------------------------------------------------------------------------

It is the exact same pipeline.
Only the training is different.

Data augmentation
-------------------------------------------------------------------------------

!!! WARNING
    TODO

Tangent propagation
-------------------------------------------------------------------------------

!!! WARNING
    TODO


Pivot
-------------------------------------------------------------------------------

!!! WARNING
    TODO



Inferno
===============================================================================

Differences with the Standard classifier pipeline
-------------------------------------------------------------------------------

!!! WARNING
    TODO



Training
-------------------------------------------------------------------------------

!!! WARNING
    TODO






<!-- <link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?"> -->
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'long'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
