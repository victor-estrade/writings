<meta charset="utf-8" emacsmode="-*- markdown -*-">

**Story 2.0**

!!! tip
    Here i'll try to write a story organizing the available materails.




New
===============================================================================



C'est quoi le contexte ? L'objet de notre étude ?
-------------------------------------------------------------------------------

> On commence par le début.

Avant les expérimentations mesuraient quelques grandeurs physiques.
Avec le développement des outils numériques les expérimentations ont gagné en complexité et mesurent de nombreuses grandeurs.
Mais il est difficile de gérer des données en haute dimension et d'en tirer des conclusions.
De plus l'augmentation de la dimensionalité s’accompagne d'une augmentation de la complexité du modèle qui doit prendre en compte de nouveaux paramètres (en particulier les réponses des nombreux appareils de mesure).
Ces paramètres n'étant pas l'objet de l'étude, ils sont qualifiés de *paramètres de nuisance* par opposition au *paramètre d'intérêt* que l'on souhaite inférer.

L'introduction de nouveaux paramètres libres entraîne une ambiguïté sur l'origine des variations des observables : d'une part le paramètres d'intérêt et d'autre par les paramètres de nuisance.
Il faut faire l'effort supplémentaire de démêler l'influence des uns et des autres.
Ce qui n'est en général pas possible à 100% et donc augmente mécaniquement l'incertitude de l'inférence.
C'est ce que l'on nomme **l'erreur systématique**.
L'erreur systématique est souvent opposé à **l'erreur statistique** qui elle provient du manque d'information sur le paramètre d'intérêt.
Ce dernier est due principalement à une faible influence du paramètre d'intérêt sur les observables (ou autres grandeurs résumées / agrégées) ou encore au manque de données.
On a alors un compromis entre le gain d'information apporté par la mesure de nouvelles observables et la perte de précision de l'inférence (du paramètre d'intérêt) due à l'introduction de nouveaux paramètres libres (nuisances).

La méthode classique, pour gérer la haute dimensionalité des données, consiste a construire à la main une ou quelques quantités agrégeant au mieux l'information que l'on cherche.
Cette méthode est limitée par les connaissances et outils théoriques et demande de la main d'oeuvre très qualifié.
Mais elle permet souvent d'obtenir des propriétés utiles comme une faible sensibilité aux paramètres de nuisance.

Opposé à la méthode classique on trouve les méthodes de réduction de dimension (machine learning).
Les réductions de dimension permettent de retenir au maximum l'information lié au paramètre d'intérêt.
Donc réduisent l'erreur statistique.
Cependant les nouvelles quantités obtenues sont souvent aussi très sensibles aux paramètres de nuisance.
Ce qui rend de démêlage difficile donc augmente l'erreur systématique.

L'objectif ici est d'étudier des méthodes pour réduire l'erreur systématique des réductions de dimension.
Plusieurs méthodes sont proposés dans la littérature :
- Devenir insensible aux effets systématique (Data augmentation, Pivotal neural network, tangent propagation)
- Profiler l'effet systématique (que l'on a écarté parce que pas le temps de tout tester non plus)
- Optimiser plus directement l'erreur totale

Nous proposons d'utiliser des architectures récentes de réseaux de neurones pour effectuer une inférence directe.
Ainsi qu'une façon de combiner cette méthode avec les méthodes précédentes pour bénéficier de leurs avantages respectifs.




C'est quoi la question ? Qu'est-ce qu'on cherche à voir ou mesurer ou vérifier ?
-------------------------------------------------------------------------------

> Dire qu'on va regarder ce qui se passe ou comparer les méthodes c'est flou. Ici on précise ce que l'on cherche.







OLD
===============================================================================

Plan
===============================================================================

- Explosion of ML in experimental science
- Improve integration of ML in complex pipeline (ML focuses a lot in classification and regression...)
- Need to measure uncertainty which is a immature field in ML

- Example with a HEP problem
- in ideal case we don't even need ML (closed formulas)
- in more realistic case (poisson + high dimensional) ML can be used to optimize stuff
- in real world nuisance parameters breaks optimality of current used Ml methods

- Robustness
- Beyond cross entropy, taking empirical distribution as input
- Even further with direct regression
- Unfortunately it does not defeat the big boss HiggsML (S << B regime)
- But if they all work together they can do better !


Version imagée brouillon pour moi
===============================================================================

Le Machine learning c'est magique. Mais certains problèmes exigent des garanties sur l'incertitude. Ce que le ML n'a pas encore.

Un de ces cas difficiles est l'analyse en HEP sur HiggsML.

Dans les cas 1D avec formule fermée un classique maximum likelihood pour l'estimation du paramètre et un Neyman construction pour l'intervalle se calculent à la main.

Dans les cas + réalistes (ou + récents) une réduction de dimension est nécessaire.
Le ML permet d'optimiser parfaitement cette réduction de dimension (Bayes classifier optimality).

Dans un cas encore + réaliste on doit gérer les paramètres de nuisance.
Et c'est difficile.

Solution 1 : régularisation à l'entrainement du ML
Solution 2 : changer la loss pour une lower bound bien trouvé (inferno)
En s'inspirant de l'architecture de la solution 2 et d'autre travaux on a ...
Solution 3 :  Regression directe

C'est bien (ça marche sur les toys) mais rien ne donne de résultats satisfaisant dans le régime faible signal (S << B).

Du coup Solution 4 = utiliser la solution 3 pour soutenir la solution 1 ou 2.
Par l'extraction de nouvelles summary statistiques très informatives sur les paramètres de nuisance il est possible de systématiquement réduire l'incertitude finale.

C'est chouette !



Version brouillon
===============================================================================

Machine learning has invaded most of the data analysis world.
The performances achievable by new methods like neural network or ensemble methods open the door to automatization of tasks previously fullfilled only by human labor.
It includes some very complex data analysis in experimental science where the amount of data and the increase of dimensionality make it impossible for human to process them efficiently.
The adoption of ML in experimental science is slowed down by mistrust in the predictions of ML models.
Accurately measure the uncertainty of ML predictions remains a very active research field.

An example embeding these problematics is the analysis of the Higgs boson property at the LHC.
The data produced by the experiments are huge and very high dimensional.

In an ideal simple case statistical inference only need a model translated to a likelihood given as a closed form formula.
In this case ML is not required at all.
For point estimators, maximum likelihood estimators gives good theorical garanties.
Standard method to compute confidence interval also benefit from theorical garanties making them unbeatable.

But in complex process the likelihood may become intractable.
It is the case for the particular case that motivates this work.
The likelihood contains millions of latent variables which requires to be marginalized through a very high dimension intergral.
This is intracktable in practice.

Fortunately a workaround using domain knowledge produces an approximative but very accurate model of the process.
The model is using a histogram counting extracted from the raw data.
Histograms does not scale with increase of dimension but increasing the dimension and the number of bins improves inference.
ML (classification) is used to find a dimension reduction keeping most of the relevant information for inference.
Theoretical garanty exists to prove that the summary statistics extracted this way is sufficient for this particular case.

Another difficulty occur in real life experiment : nuisance parameters / systematic effects.
Taking them into account is required to avoid biased results although it increases uncertainty.
Nuisance parameters breaks the theoretical garanties of optimality of classification.

Several regularization have been proposed to helps mitigate the increase of the final uncertainty comming from nuisance paramters.
Some methods (inferno) took a step further by optimizing a neural network directly on a lower bound of the final uncertainty.
These methods benefits from the flexibility of neural network architectures to take as input empirical distributions instead of samples.

Pushing this further a direct regression of the parameter of interest is possible.
This 3rd method is likelihood free and is way faster to compute an inference.

But none of these methods performs very well is the rare signal regime (S << B).

Using the best of both world a final proposition is to extract more summary statistics from the high dimensional data.
New summary statistics giving information about the nuisance parameters could indeed be extracted.
Doing so complete the approximative model yielding better results in all case.


Short version
===============================================================================






Long version
===============================================================================


Subsection
-------------------------------------------------------------------------------



<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/dark.css?">
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script>markdeepOptions={tocStyle:'none'};</script>
<!-- Markdeep: --><script src="markdeep.min.js"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
